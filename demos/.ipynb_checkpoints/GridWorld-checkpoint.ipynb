{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ProgressBars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReinforcementLearning.FiniteMarkovDecisionProcesses as MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states = [\n",
    "    # 1 (up)  2 (down)  3 (right)  4 (left) \n",
    "      1       5         2         15         #  1\n",
    "      2       6         3          1         #  2 \n",
    "      3       7         3          2         #  3\n",
    "     15       8         5          4         #  4\n",
    "      1       9         6          4         #  5\n",
    "      2      10         7          5         #  6\n",
    "      3      11         7          6         #  7\n",
    "      4      12         9          8         #  8\n",
    "      5      13        10          8         #  9\n",
    "      6      14        11          9         # 10\n",
    "      7      15        11         10         # 11\n",
    "      8      12        13         12         # 12\n",
    "      9      13        14         12         # 13\n",
    "     10      14        15         13         # 14\n",
    "     15      15        15         15         # 15\n",
    "]\n",
    "\n",
    "rewards = -1*ones(size(next_states))\n",
    "rewards[15, :] .= 0\n",
    "\n",
    "fmdp = MDP.DeterministicFiniteMDP((s, a) -> next_states[s, a], (s, a) -> rewards[s, a], 4, 15, 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_random_policy = 0.25 * ones(size(next_states)...)\n",
    "V_uniform_random_policy = [-14.0, -20.0, -22.0, -14.0, -18.0, -20.0, -20.0, -20.0, -20.0, -18.0, -14.0, -22.0, -20.0, -14.0, 0.0];\n",
    "\n",
    "random_deterministic_policy = rand(1:4, size(next_states, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_V = [-1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Handy Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " 15.0   1.0   2.0   3.0\n",
       "  4.0   5.0   6.0   7.0\n",
       "  8.0   9.0  10.0  11.0\n",
       " 12.0  13.0  14.0  15.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function V_to_matrix_form(V)\n",
    "    VV = Matrix{Float64}(undef, 4, 4)\n",
    "    VV[1,1] = V[15]\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        ℓ = rem(i,4)\n",
    "        VV[k+1, ℓ+1] = V[i]\n",
    "    end\n",
    "    VV[4,4] = VV[1,1]\n",
    "    return VV\n",
    "end\n",
    "\n",
    "V_test = 1:15\n",
    "V_to_matrix_form(V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_to_char (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function action_to_char(a::Integer)\n",
    "    if a == 1 # up\n",
    "        return '↑'\n",
    "    elseif a == 2 # down\n",
    "        return '↓'\n",
    "    elseif a == 3 # right\n",
    "        return '→'\n",
    "    elseif a == 4 # left\n",
    "        return '←'\n",
    "    else\n",
    "        return '?'\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Char}:\n",
       " '█'  '↑'  '↑'  '↓'\n",
       " '→'  '→'  '←'  '→'\n",
       " '→'  '→'  '↑'  '←'\n",
       " '↓'  '↑'  '↓'  '█'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function 𝐩_to_matrix_form(𝐩)\n",
    "    PP = Matrix{Char}(undef, 4, 4)\n",
    "    PP[1,1] = '█'\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        ℓ = rem(i,4)\n",
    "        PP[k+1, ℓ+1] = action_to_char(𝐩[i])\n",
    "    end\n",
    "    PP[4,4] = '█'\n",
    "    return PP\n",
    "end\n",
    "\n",
    "𝐩_to_matrix_form(random_deterministic_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_probabilities_to_char (generic function with 2 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function action_probabilities_to_char(𝕡::AbstractVector{<:Real})\n",
    "    vert = ' '\n",
    "    if 𝕡[1] > 0 && 𝕡[2] > 0 # both UP and DOWN are valid\n",
    "        vert = '↕'\n",
    "    elseif 𝕡[1] > 0 # only UP is valid\n",
    "        vert = '↑'\n",
    "    elseif 𝕡[2] > 0 # only DOWN is valid\n",
    "        vert = '↓'\n",
    "    end\n",
    "    horz = ' '\n",
    "    if 𝕡[3] > 0 && 𝕡[4] > 0 # both RIGHT and LEFT are valid\n",
    "        horz = '↔'\n",
    "    elseif 𝕡[3] > 0 # only RIGHT is valid\n",
    "        horz = '→'\n",
    "    elseif 𝕡[4] > 0 # only LEFT is valid\n",
    "        horz = '←'\n",
    "    end\n",
    "    return \"$vert$horz\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{String}:\n",
       " \"█\"   \"↕↔\"  \"↕↔\"  \"↕↔\"\n",
       " \"↕↔\"  \"↕↔\"  \"↕↔\"  \"↕↔\"\n",
       " \"↕↔\"  \"↕↔\"  \"↕↔\"  \"↕↔\"\n",
       " \"↕↔\"  \"↕↔\"  \"↕↔\"  \"█\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function 𝐏_to_matrix_form(𝐏)\n",
    "    PP = Matrix{String}(undef, 4, 4)\n",
    "    PP[1,1] =\"█\"\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        ℓ = rem(i,4)\n",
    "        PP[k+1, ℓ+1] = action_probabilities_to_char(𝐏[i,:])\n",
    "    end\n",
    "    PP[4,4] = \"█\"\n",
    "    return PP\n",
    "end\n",
    "\n",
    "𝐏_to_matrix_form(uniform_random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_errors_in_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_errors_in_policy(𝐩::AbstractVector{<:Integer}, 𝐏::AbstractMatrix{<:Real})\n",
    "    errors = []\n",
    "    for s in 1:length(𝐩)\n",
    "        if 𝐏[s, 𝐩[s]] == 0\n",
    "            push!(errors, s)\n",
    "        end\n",
    "    end\n",
    "    return errors\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: lenght not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lenght not defined",
      "",
      "Stacktrace:",
      " [1] find_errors_in_policy(𝐩::Vector{Int64}, 𝐏::Matrix{Float64})",
      "   @ Main .\\In[27]:3",
      " [2] 𝐩_to_matrix_form_with_errors(𝐩::Vector{Int64}, 𝐏::Matrix{Float64})",
      "   @ Main .\\In[28]:17",
      " [3] top-level scope",
      "   @ In[28]:27",
      " [4] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "function error_symbol(ch)\n",
    "    if ch == '↑'\n",
    "        return '⍐'\n",
    "    elseif ch == '↓'\n",
    "        return '⍗'\n",
    "    elseif ch == '→'\n",
    "        return '⍈'\n",
    "    elseif ch == '←'\n",
    "        return '⍇'\n",
    "    else\n",
    "        return ' '\n",
    "    end\n",
    "end\n",
    "\n",
    "function 𝐩_to_matrix_form_with_errors(𝐩, 𝐏)\n",
    "    PP = 𝐩_to_matrix_form(𝐩)\n",
    "    errors = find_errors_in_policy(𝐩, 𝐏)\n",
    "    for err in errors\n",
    "        k = div(err, 4)\n",
    "        ℓ = rem(err, 4)\n",
    "        PP[k+1, ℓ+1] = error_symbol(PP[k+1, ℓ+1])\n",
    "    end\n",
    "    return PP\n",
    "end\n",
    "\n",
    "𝐏 = zeros(15, 4)\n",
    "𝐩_to_matrix_form_with_errors(random_deterministic_policy, 𝐏)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP: Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation via \"Textbook\" Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐏 = copy(uniform_random_policy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "iters_no, converged = MDP.dp_evaluate_policy_textbook!(V, Q, fmdp, 𝐏, 1.0; tol = 1e-4, maxiter = 1000)\n",
    "\n",
    "ΔV = abs.(V - V_uniform_random_policy)\n",
    "\n",
    "print(\"Achieved error = $(max(ΔV...)), after $iters_no iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Δ = V_ -> max.((abs.(V_ - V_uniform_random_policy))...)\n",
    "\n",
    "iters_no = 1000\n",
    "\n",
    "δ = zeros(iters_no+1)\n",
    "Δ = zeros(iters_no+1)\n",
    "\n",
    "δ[1] = NaN\n",
    "Δ[1] = compute_Δ(V)\n",
    "\n",
    "for i in 1:iters_no\n",
    "    MDP.Q_from_V!(Q, V, fmdp, 1.0)\n",
    "    δ[i+1] = MDP.V_from_Q!(V, Q, 𝐏)\n",
    "    Δ[i+1] = compute_Δ(V)\n",
    "end # for: iterations\n",
    "\n",
    "Δ_textbook = copy(Δ)\n",
    "\n",
    "l = @layout [a b]\n",
    "p1 = plot(Δ, label=\"error\")\n",
    "plot!(δ, label=\"incremental diff.\")\n",
    "p2 = plot(log10.(Δ), label=\"error\")\n",
    "plot!(log10.(δ), label=\"incremental diff.\")\n",
    "plot(p1, p2, layout = l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_dp_random_policy = copy(V)\n",
    "V_to_matrix_form(round.(V_dp_random_policy; digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = MDP.allocate_𝐩(Q)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation using in-place update of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐏 = copy(uniform_random_policy)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Δ = V_ -> max.((abs.(V_ - V_uniform_random_policy))...)\n",
    "\n",
    "iters_no = 1000\n",
    "\n",
    "δ = zeros(iters_no+1)\n",
    "Δ = zeros(iters_no+1)\n",
    "\n",
    "δ[1] = NaN\n",
    "Δ[1] = compute_Δ(V)\n",
    "\n",
    "for i in 1:iters_no\n",
    "    MDP.Q_from_V!(Q, V, fmdp, 1.0)\n",
    "    δ[i+1] = MDP.dp_update_V!(V, fmdp, 𝐏, 1.0)\n",
    "    Δ[i+1] = compute_Δ(V)\n",
    "end # for: iterations\n",
    "\n",
    "Δ_Vinplace = copy(Δ)\n",
    "\n",
    "l = @layout [a b]\n",
    "p1 = plot(Δ, label=\"error\")\n",
    "plot!(δ, label=\"incremental diff.\")\n",
    "p2 = plot(log10.(Δ), label=\"error\")\n",
    "plot!(log10.(δ), label=\"incremental diff.\")\n",
    "plot(p1, p2, layout = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx = findfirst(δ .≤ 1e-4)\n",
    "print(\"Achieved error = $(max(ΔV...)), after $(ndx-1) iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation - Comparison of different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = @layout [a b]\n",
    "p1 = plot(Δ_textbook, label=\"textbook\")\n",
    "plot!(Δ_Vinplace, label=\"V in-place\")\n",
    "p2 = plot(log10.(Δ_textbook), label=\"textbook\")\n",
    "plot!(log10.(Δ_Vinplace), label=\"V in-place\")\n",
    "plot(p1, p2, layout = l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP: Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Optimization using \"textbook\" **GPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_textbook_GPI(; tol=1e-4, maxiter=5)\n",
    "    𝐩 = copy(random_deterministic_policy)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "    V[MDP.terminal_state(fmdp)] = 0\n",
    "    converged = false\n",
    "    iters_no = 1000\n",
    "    for i = 1:1000\n",
    "        MDP.dp_evaluate_policy_textbook!(V, Q, fmdp, 𝐩, 1.0; tol = tol, maxiter = maxiter)\n",
    "        modified = MDP.𝐩_from_Q!(𝐩, Q)\n",
    "        if !modified\n",
    "            iters_no = i\n",
    "            converged = true\n",
    "            break\n",
    "        end\n",
    "    end # for: iterations\n",
    "    return 𝐩, V, iters_no\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩, V, iters_no = optimize_policy_textbook_GPI(tol=1e-12, maxiter=5)\n",
    "\n",
    "ΔV = abs.((V - optimal_V))\n",
    "err = max(ΔV...)\n",
    "\n",
    "print(\"DP: policy optimization (textbook GPI): max abs err = $err (iters no = $iters_no)\")\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = [1, 5, 10, 20, 30, 50, 75, 100]\n",
    "total_iter = NaN * ones(size(maxiter))\n",
    "\n",
    "for (i, iter) in enumerate(maxiter)\n",
    "    𝐩, V, iters_no = optimize_policy_textbook_GPI(tol=1e-12, maxiter=iter)\n",
    "\n",
    "    total_iter[i] = iters_no * iter\n",
    "    ΔV = abs.((V - optimal_V))\n",
    "    err = max(ΔV...)\n",
    "    println(\"DP: policy optimization (textbook GPI with maxiter=$iter): iters no = $iters_no (outer), $(iters_no * iter) (total)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iter_textbook = total_iter\n",
    "plot(maxiter, total_iter, linestyle=:dash, marker=:circle, label=\"total iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Optimization using in-place update of V (Value Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_V_inplace(; tol=1e-12, maxiter=1000)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "    converged = false\n",
    "    iters_no = maxiter\n",
    "    for i = 1:maxiter\n",
    "        Δ = MDP.dp_update_V!(V, fmdp, 1.0)\n",
    "        if Δ < tol\n",
    "            converged = true\n",
    "            iters_no = i\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, iters_no\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, iters_no = optimize_policy_V_inplace(tol=1e-12, maxiter=1000)\n",
    "ΔV = abs.((V - optimal_V))\n",
    "err = max(ΔV...)\n",
    "println(\"DP: policy optimization (value iteration): max abs err = $err (iters no = $iters_no)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Karlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MK: Baseline implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline MK: Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_policy_MK(; maxiter = 10000)\n",
    "    simulator = MDP.create_simulator_from_policy(fmdp, uniform_random_policy, 1000)\n",
    "\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "\n",
    "    MDP.mk_evaluate_policy!(Q, simulator, 1.0; maxiter = maxiter)\n",
    "\n",
    "    MDP.V_from_Q!(V, Q, 𝐏)\n",
    "    return V\n",
    "end\n",
    "\n",
    "function evaluate_policy_MK_repeated(; maxiter = 10000, n_attempts = 5, verbose = true)\n",
    "    total_err = 0.0\n",
    "    for _ in 1:n_attempts\n",
    "        V = evaluate_policy_MK(maxiter = maxiter)\n",
    "        ΔV = abs.(V - V_uniform_random_policy)\n",
    "        err = max(ΔV...)\n",
    "        if verbose\n",
    "            println(\"MK: policy evaluation (maxiter = $maxiter): max abs err = $err\")\n",
    "        end\n",
    "        total_err += err\n",
    "    end\n",
    "    total_err /= n_attempts\n",
    "    return total_err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = evaluate_policy_MK_repeated(maxiter = 10000, n_attempts = 10, verbose = true)\n",
    "println(\"---\")\n",
    "println(\"MK: policy evaluation: mean max abs err = $total_err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = [1000, 5000, 10000, 25000, 50000, 100000]\n",
    "mean_error = NaN*ones(size(maxiter))\n",
    "\n",
    "for (i, iter) in enumerate(maxiter)\n",
    "    mean_error[i] = evaluate_policy_MK_repeated(maxiter = iter, n_attempts = 5, verbose = false)\n",
    "    println(\"MK: policy evaluation: mean max abs err after $iter iters. : $(mean_error[i])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log10.(maxiter), mean_error, linestyle=:dash, marker=:circle, label=\"mean error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline MK: Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = copy(random_deterministic_policy)\n",
    "simulator = MDP.create_simulator_from_policy(fmdp, 𝐩, 0.05, 100)\n",
    "\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "\n",
    "maxiter = 1000\n",
    "iters_no = maxiter\n",
    "for i = 1:maxiter\n",
    "    MDP.mk_evaluate_policy!(Q, simulator, 1.0; maxiter = 10000)\n",
    "    modified = MDP.𝐩_from_Q!(𝐩, Q)\n",
    "    if !modified\n",
    "        iters_no = i\n",
    "        break\n",
    "    end\n",
    "end # for: iterations\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q, 𝐩)\n",
    "ΔV = abs.(V - optimal_V)\n",
    "err = max(ΔV...)\n",
    "println(\"MK: policy optimization: max abs err = $err after $iters_no iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MK: Incremental implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental MK: Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐏 = copy(uniform_random_policy)\n",
    "simulator = MDP.create_simulator_from_policy(fmdp, 𝐏, 1000)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "\n",
    "ΔQ = -Inf\n",
    "for _ = 1:10000\n",
    "    ΔQ = MDP.mk_update_Q!(Q, 0.99, simulator, 1.0)\n",
    "end\n",
    "\n",
    "MDP.V_from_Q!(V, Q, 𝐏)\n",
    "ΔV = abs.((V - V_uniform_random_policy))\n",
    "err = max(ΔV...)\n",
    "println(\"MK: policy evaluation (incremental): max abs err = $err (final ΔQ = $ΔQ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDP.V_from_Q!(V, Q, 𝐏)\n",
    "V_to_matrix_form(round.(V; digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental MK: Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = MDP.allocate_Q(fmdp)\n",
    "simulator = MDP.create_simulator_from_Q(fmdp, Q, 0.05, 1000)\n",
    "\n",
    "ΔQ = -Inf\n",
    "for i = 1:10000\n",
    "    ΔQ = MDP.mk_update_Q!(Q, 0.95, simulator, 1.0)\n",
    "end # for: iterations\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q)\n",
    "ΔV = abs.(V - optimal_V)\n",
    "err = max(ΔV...)\n",
    "print(\"MK: policy optimization (incremental): max abs err = $err (final ΔQ = $ΔQ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = MDP.allocate_𝐩(Q)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_MK_incremental(; γ = 0.95, ε = 0.05)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "    simulator = MDP.create_simulator_from_Q(fmdp, Q, ε, 1000)\n",
    "\n",
    "    𝐩 = MDP.allocate_𝐩(Q)\n",
    "\n",
    "    ΔQ = -Inf\n",
    "    maxiter = 10000\n",
    "    iters_no = maxiter\n",
    "    for i = 1:maxiter\n",
    "        ΔQ = MDP.mk_update_Q!(Q, γ, simulator, 1.0)\n",
    "        MDP.𝐩_from_Q!(𝐩, Q)\n",
    "        #! It does NOT WORK if we only check whether policy is changed or not!\n",
    "        if ΔQ < 1e-4\n",
    "            iters_no = i\n",
    "            break\n",
    "        end\n",
    "    end # for: iterations\n",
    "    return iters_no, Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_no, Q = optimize_policy_MK_incremental(γ = 0.95, ε = 0.05)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q)\n",
    "ΔV = abs.(V - optimal_V)\n",
    "err = max(ΔV...)\n",
    "print(\"MK: policy optimization (incremental): max abs err = $err (final ΔQ = $ΔQ) after $iters_no iters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_to_matrix_form(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = MDP.allocate_𝐩(Q)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ_array = [0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "for γ in γ_array\n",
    "    iters_no, Q = optimize_policy_MK_incremental(γ = γ, ε = 0.05)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    MDP.V_from_Q!(V, Q)\n",
    "    ΔV = abs.(V - optimal_V)\n",
    "    err = max(ΔV...)\n",
    "    println(\"MK: policy optimization (incremental γ = $γ): max abs err = $err after $iters_no iters.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = MDP.allocate_𝐩(Q)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Δ = V_ -> max.((abs.(V_ - optimal_V))...)\n",
    "\n",
    "simulator = MDP.create_simulator_from_Q(fmdp, Q, 0.05, 1000)\n",
    "\n",
    "maxiter = 10000\n",
    "iters_no = maxiter\n",
    "\n",
    "δ = zeros(iters_no+1)\n",
    "Δ = zeros(iters_no+1)\n",
    "\n",
    "δ[1] = NaN\n",
    "Δ[1] = compute_Δ(V)\n",
    "\n",
    "for i in 1:maxiter\n",
    "    δ[i+1] = MDP.mk_update_Q!(Q, 0.95, simulator, 1.0)\n",
    "    MDP.V_from_Q!(V, Q)\n",
    "    Δ[i+1] = compute_Δ(V)\n",
    "end # for: iterations\n",
    "\n",
    "plot(Δ, label=\"error\")\n",
    "plot!(δ, label=\"incremental diff.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝐩 = MDP.allocate_𝐩(Q)\n",
    "MDP.𝐩_from_Q!(𝐩, Q)\n",
    "𝐩_to_matrix_form(𝐩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
