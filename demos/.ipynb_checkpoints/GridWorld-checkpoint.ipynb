{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using ProgressBars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReinforcementLearning.FiniteMarkovDecisionProcesses as MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states = [\n",
    "    # 1 (up)  2 (down)  3 (right)  4 (left) \n",
    "      1       5         2         15         #  1\n",
    "      2       6         3          1         #  2 \n",
    "      3       7         3          2         #  3\n",
    "     15       8         5          4         #  4\n",
    "      1       9         6          4         #  5\n",
    "      2      10         7          5         #  6\n",
    "      3      11         7          6         #  7\n",
    "      4      12         9          8         #  8\n",
    "      5      13        10          8         #  9\n",
    "      6      14        11          9         # 10\n",
    "      7      15        11         10         # 11\n",
    "      8      12        13         12         # 12\n",
    "      9      13        14         12         # 13\n",
    "     10      14        15         13         # 14\n",
    "     15      15        15         15         # 15\n",
    "]\n",
    "\n",
    "rewards = -1*ones(size(next_states))\n",
    "rewards[15, :] .= 0\n",
    "\n",
    "fmdp = MDP.DeterministicFiniteMDP((s, a) -> next_states[s, a], (s, a) -> rewards[s, a], 4, 15, 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_random_policy = 0.25 * ones(size(next_states)...)\n",
    "V_uniform_random_policy = [-14.0, -20.0, -22.0, -14.0, -18.0, -20.0, -20.0, -20.0, -20.0, -18.0, -14.0, -22.0, -20.0, -14.0, 0.0];\n",
    "\n",
    "random_deterministic_policy = rand(1:4, size(next_states, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_V = [-1.0, -2.0, -3.0, -1.0, -2.0, -3.0, -2.0, -2.0, -3.0, -2.0, -1.0, -3.0, -2.0, -1.0, 0.0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Handy Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4Ã—4 Matrix{Float64}:\n",
       " 15.0   1.0   2.0   3.0\n",
       "  4.0   5.0   6.0   7.0\n",
       "  8.0   9.0  10.0  11.0\n",
       " 12.0  13.0  14.0  15.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function V_to_matrix_form(V)\n",
    "    VV = Matrix{Float64}(undef, 4, 4)\n",
    "    VV[1,1] = V[15]\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        â„“ = rem(i,4)\n",
    "        VV[k+1, â„“+1] = V[i]\n",
    "    end\n",
    "    VV[4,4] = VV[1,1]\n",
    "    return VV\n",
    "end\n",
    "\n",
    "V_test = 1:15\n",
    "V_to_matrix_form(V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_to_char (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function action_to_char(a::Integer)\n",
    "    if a == 1 # up\n",
    "        return 'â†‘'\n",
    "    elseif a == 2 # down\n",
    "        return 'â†“'\n",
    "    elseif a == 3 # right\n",
    "        return 'â†’'\n",
    "    elseif a == 4 # left\n",
    "        return 'â†'\n",
    "    else\n",
    "        return '?'\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4Ã—4 Matrix{Char}:\n",
       " 'â–ˆ'  'â†‘'  'â†‘'  'â†“'\n",
       " 'â†’'  'â†’'  'â†'  'â†’'\n",
       " 'â†’'  'â†’'  'â†‘'  'â†'\n",
       " 'â†“'  'â†‘'  'â†“'  'â–ˆ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ğ©_to_matrix_form(ğ©)\n",
    "    PP = Matrix{Char}(undef, 4, 4)\n",
    "    PP[1,1] = 'â–ˆ'\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        â„“ = rem(i,4)\n",
    "        PP[k+1, â„“+1] = action_to_char(ğ©[i])\n",
    "    end\n",
    "    PP[4,4] = 'â–ˆ'\n",
    "    return PP\n",
    "end\n",
    "\n",
    "ğ©_to_matrix_form(random_deterministic_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action_probabilities_to_char (generic function with 2 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function action_probabilities_to_char(ğ•¡::AbstractVector{<:Real})\n",
    "    vert = ' '\n",
    "    if ğ•¡[1] > 0 && ğ•¡[2] > 0 # both UP and DOWN are valid\n",
    "        vert = 'â†•'\n",
    "    elseif ğ•¡[1] > 0 # only UP is valid\n",
    "        vert = 'â†‘'\n",
    "    elseif ğ•¡[2] > 0 # only DOWN is valid\n",
    "        vert = 'â†“'\n",
    "    end\n",
    "    horz = ' '\n",
    "    if ğ•¡[3] > 0 && ğ•¡[4] > 0 # both RIGHT and LEFT are valid\n",
    "        horz = 'â†”'\n",
    "    elseif ğ•¡[3] > 0 # only RIGHT is valid\n",
    "        horz = 'â†’'\n",
    "    elseif ğ•¡[4] > 0 # only LEFT is valid\n",
    "        horz = 'â†'\n",
    "    end\n",
    "    return \"$vert$horz\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4Ã—4 Matrix{String}:\n",
       " \"â–ˆ\"   \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"\n",
       " \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"\n",
       " \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"\n",
       " \"â†•â†”\"  \"â†•â†”\"  \"â†•â†”\"  \"â–ˆ\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ğ_to_matrix_form(ğ)\n",
    "    PP = Matrix{String}(undef, 4, 4)\n",
    "    PP[1,1] =\"â–ˆ\"\n",
    "    for i = 1:14\n",
    "        k = div(i,4)\n",
    "        â„“ = rem(i,4)\n",
    "        PP[k+1, â„“+1] = action_probabilities_to_char(ğ[i,:])\n",
    "    end\n",
    "    PP[4,4] = \"â–ˆ\"\n",
    "    return PP\n",
    "end\n",
    "\n",
    "ğ_to_matrix_form(uniform_random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_errors_in_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_errors_in_policy(ğ©::AbstractVector{<:Integer}, ğ::AbstractMatrix{<:Real})\n",
    "    errors = []\n",
    "    for s in 1:length(ğ©)\n",
    "        if ğ[s, ğ©[s]] == 0\n",
    "            push!(errors, s)\n",
    "        end\n",
    "    end\n",
    "    return errors\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: lenght not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lenght not defined",
      "",
      "Stacktrace:",
      " [1] find_errors_in_policy(ğ©::Vector{Int64}, ğ::Matrix{Float64})",
      "   @ Main .\\In[27]:3",
      " [2] ğ©_to_matrix_form_with_errors(ğ©::Vector{Int64}, ğ::Matrix{Float64})",
      "   @ Main .\\In[28]:17",
      " [3] top-level scope",
      "   @ In[28]:27",
      " [4] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "function error_symbol(ch)\n",
    "    if ch == 'â†‘'\n",
    "        return 'â'\n",
    "    elseif ch == 'â†“'\n",
    "        return 'â—'\n",
    "    elseif ch == 'â†’'\n",
    "        return 'âˆ'\n",
    "    elseif ch == 'â†'\n",
    "        return 'â‡'\n",
    "    else\n",
    "        return ' '\n",
    "    end\n",
    "end\n",
    "\n",
    "function ğ©_to_matrix_form_with_errors(ğ©, ğ)\n",
    "    PP = ğ©_to_matrix_form(ğ©)\n",
    "    errors = find_errors_in_policy(ğ©, ğ)\n",
    "    for err in errors\n",
    "        k = div(err, 4)\n",
    "        â„“ = rem(err, 4)\n",
    "        PP[k+1, â„“+1] = error_symbol(PP[k+1, â„“+1])\n",
    "    end\n",
    "    return PP\n",
    "end\n",
    "\n",
    "ğ = zeros(15, 4)\n",
    "ğ©_to_matrix_form_with_errors(random_deterministic_policy, ğ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP: Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation via \"Textbook\" Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ = copy(uniform_random_policy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "iters_no, converged = MDP.dp_evaluate_policy_textbook!(V, Q, fmdp, ğ, 1.0; tol = 1e-4, maxiter = 1000)\n",
    "\n",
    "Î”V = abs.(V - V_uniform_random_policy)\n",
    "\n",
    "print(\"Achieved error = $(max(Î”V...)), after $iters_no iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Î” = V_ -> max.((abs.(V_ - V_uniform_random_policy))...)\n",
    "\n",
    "iters_no = 1000\n",
    "\n",
    "Î´ = zeros(iters_no+1)\n",
    "Î” = zeros(iters_no+1)\n",
    "\n",
    "Î´[1] = NaN\n",
    "Î”[1] = compute_Î”(V)\n",
    "\n",
    "for i in 1:iters_no\n",
    "    MDP.Q_from_V!(Q, V, fmdp, 1.0)\n",
    "    Î´[i+1] = MDP.V_from_Q!(V, Q, ğ)\n",
    "    Î”[i+1] = compute_Î”(V)\n",
    "end # for: iterations\n",
    "\n",
    "Î”_textbook = copy(Î”)\n",
    "\n",
    "l = @layout [a b]\n",
    "p1 = plot(Î”, label=\"error\")\n",
    "plot!(Î´, label=\"incremental diff.\")\n",
    "p2 = plot(log10.(Î”), label=\"error\")\n",
    "plot!(log10.(Î´), label=\"incremental diff.\")\n",
    "plot(p1, p2, layout = l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_dp_random_policy = copy(V)\n",
    "V_to_matrix_form(round.(V_dp_random_policy; digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = MDP.allocate_ğ©(Q)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation using in-place update of V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ = copy(uniform_random_policy)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Î” = V_ -> max.((abs.(V_ - V_uniform_random_policy))...)\n",
    "\n",
    "iters_no = 1000\n",
    "\n",
    "Î´ = zeros(iters_no+1)\n",
    "Î” = zeros(iters_no+1)\n",
    "\n",
    "Î´[1] = NaN\n",
    "Î”[1] = compute_Î”(V)\n",
    "\n",
    "for i in 1:iters_no\n",
    "    MDP.Q_from_V!(Q, V, fmdp, 1.0)\n",
    "    Î´[i+1] = MDP.dp_update_V!(V, fmdp, ğ, 1.0)\n",
    "    Î”[i+1] = compute_Î”(V)\n",
    "end # for: iterations\n",
    "\n",
    "Î”_Vinplace = copy(Î”)\n",
    "\n",
    "l = @layout [a b]\n",
    "p1 = plot(Î”, label=\"error\")\n",
    "plot!(Î´, label=\"incremental diff.\")\n",
    "p2 = plot(log10.(Î”), label=\"error\")\n",
    "plot!(log10.(Î´), label=\"incremental diff.\")\n",
    "plot(p1, p2, layout = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx = findfirst(Î´ .â‰¤ 1e-4)\n",
    "print(\"Achieved error = $(max(Î”V...)), after $(ndx-1) iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Evaluation - Comparison of different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = @layout [a b]\n",
    "p1 = plot(Î”_textbook, label=\"textbook\")\n",
    "plot!(Î”_Vinplace, label=\"V in-place\")\n",
    "p2 = plot(log10.(Î”_textbook), label=\"textbook\")\n",
    "plot!(log10.(Î”_Vinplace), label=\"V in-place\")\n",
    "plot(p1, p2, layout = l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP: Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Optimization using \"textbook\" **GPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_textbook_GPI(; tol=1e-4, maxiter=5)\n",
    "    ğ© = copy(random_deterministic_policy)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "    V[MDP.terminal_state(fmdp)] = 0\n",
    "    converged = false\n",
    "    iters_no = 1000\n",
    "    for i = 1:1000\n",
    "        MDP.dp_evaluate_policy_textbook!(V, Q, fmdp, ğ©, 1.0; tol = tol, maxiter = maxiter)\n",
    "        modified = MDP.ğ©_from_Q!(ğ©, Q)\n",
    "        if !modified\n",
    "            iters_no = i\n",
    "            converged = true\n",
    "            break\n",
    "        end\n",
    "    end # for: iterations\n",
    "    return ğ©, V, iters_no\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ©, V, iters_no = optimize_policy_textbook_GPI(tol=1e-12, maxiter=5)\n",
    "\n",
    "Î”V = abs.((V - optimal_V))\n",
    "err = max(Î”V...)\n",
    "\n",
    "print(\"DP: policy optimization (textbook GPI): max abs err = $err (iters no = $iters_no)\")\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = [1, 5, 10, 20, 30, 50, 75, 100]\n",
    "total_iter = NaN * ones(size(maxiter))\n",
    "\n",
    "for (i, iter) in enumerate(maxiter)\n",
    "    ğ©, V, iters_no = optimize_policy_textbook_GPI(tol=1e-12, maxiter=iter)\n",
    "\n",
    "    total_iter[i] = iters_no * iter\n",
    "    Î”V = abs.((V - optimal_V))\n",
    "    err = max(Î”V...)\n",
    "    println(\"DP: policy optimization (textbook GPI with maxiter=$iter): iters no = $iters_no (outer), $(iters_no * iter) (total)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iter_textbook = total_iter\n",
    "plot(maxiter, total_iter, linestyle=:dash, marker=:circle, label=\"total iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DP: Policy Optimization using in-place update of V (Value Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_V_inplace(; tol=1e-12, maxiter=1000)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "    converged = false\n",
    "    iters_no = maxiter\n",
    "    for i = 1:maxiter\n",
    "        Î” = MDP.dp_update_V!(V, fmdp, 1.0)\n",
    "        if Î” < tol\n",
    "            converged = true\n",
    "            iters_no = i\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return V, iters_no\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, iters_no = optimize_policy_V_inplace(tol=1e-12, maxiter=1000)\n",
    "Î”V = abs.((V - optimal_V))\n",
    "err = max(Î”V...)\n",
    "println(\"DP: policy optimization (value iteration): max abs err = $err (iters no = $iters_no)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Karlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MK: Baseline implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline MK: Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_policy_MK(; maxiter = 10000)\n",
    "    simulator = MDP.create_simulator_from_policy(fmdp, uniform_random_policy, 1000)\n",
    "\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "\n",
    "    MDP.mk_evaluate_policy!(Q, simulator, 1.0; maxiter = maxiter)\n",
    "\n",
    "    MDP.V_from_Q!(V, Q, ğ)\n",
    "    return V\n",
    "end\n",
    "\n",
    "function evaluate_policy_MK_repeated(; maxiter = 10000, n_attempts = 5, verbose = true)\n",
    "    total_err = 0.0\n",
    "    for _ in 1:n_attempts\n",
    "        V = evaluate_policy_MK(maxiter = maxiter)\n",
    "        Î”V = abs.(V - V_uniform_random_policy)\n",
    "        err = max(Î”V...)\n",
    "        if verbose\n",
    "            println(\"MK: policy evaluation (maxiter = $maxiter): max abs err = $err\")\n",
    "        end\n",
    "        total_err += err\n",
    "    end\n",
    "    total_err /= n_attempts\n",
    "    return total_err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = evaluate_policy_MK_repeated(maxiter = 10000, n_attempts = 10, verbose = true)\n",
    "println(\"---\")\n",
    "println(\"MK: policy evaluation: mean max abs err = $total_err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = [1000, 5000, 10000, 25000, 50000, 100000]\n",
    "mean_error = NaN*ones(size(maxiter))\n",
    "\n",
    "for (i, iter) in enumerate(maxiter)\n",
    "    mean_error[i] = evaluate_policy_MK_repeated(maxiter = iter, n_attempts = 5, verbose = false)\n",
    "    println(\"MK: policy evaluation: mean max abs err after $iter iters. : $(mean_error[i])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log10.(maxiter), mean_error, linestyle=:dash, marker=:circle, label=\"mean error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline MK: Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = copy(random_deterministic_policy)\n",
    "simulator = MDP.create_simulator_from_policy(fmdp, ğ©, 0.05, 100)\n",
    "\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "\n",
    "maxiter = 1000\n",
    "iters_no = maxiter\n",
    "for i = 1:maxiter\n",
    "    MDP.mk_evaluate_policy!(Q, simulator, 1.0; maxiter = 10000)\n",
    "    modified = MDP.ğ©_from_Q!(ğ©, Q)\n",
    "    if !modified\n",
    "        iters_no = i\n",
    "        break\n",
    "    end\n",
    "end # for: iterations\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q, ğ©)\n",
    "Î”V = abs.(V - optimal_V)\n",
    "err = max(Î”V...)\n",
    "println(\"MK: policy optimization: max abs err = $err after $iters_no iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MK: Incremental implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental MK: Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ = copy(uniform_random_policy)\n",
    "simulator = MDP.create_simulator_from_policy(fmdp, ğ, 1000)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "\n",
    "Î”Q = -Inf\n",
    "for _ = 1:10000\n",
    "    Î”Q = MDP.mk_update_Q!(Q, 0.99, simulator, 1.0)\n",
    "end\n",
    "\n",
    "MDP.V_from_Q!(V, Q, ğ)\n",
    "Î”V = abs.((V - V_uniform_random_policy))\n",
    "err = max(Î”V...)\n",
    "println(\"MK: policy evaluation (incremental): max abs err = $err (final Î”Q = $Î”Q)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDP.V_from_Q!(V, Q, ğ)\n",
    "V_to_matrix_form(round.(V; digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental MK: Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = MDP.allocate_Q(fmdp)\n",
    "simulator = MDP.create_simulator_from_Q(fmdp, Q, 0.05, 1000)\n",
    "\n",
    "Î”Q = -Inf\n",
    "for i = 1:10000\n",
    "    Î”Q = MDP.mk_update_Q!(Q, 0.95, simulator, 1.0)\n",
    "end # for: iterations\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q)\n",
    "Î”V = abs.(V - optimal_V)\n",
    "err = max(Î”V...)\n",
    "print(\"MK: policy optimization (incremental): max abs err = $err (final Î”Q = $Î”Q)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = MDP.allocate_ğ©(Q)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function optimize_policy_MK_incremental(; Î³ = 0.95, Îµ = 0.05)\n",
    "    Q = MDP.allocate_Q(fmdp)\n",
    "    simulator = MDP.create_simulator_from_Q(fmdp, Q, Îµ, 1000)\n",
    "\n",
    "    ğ© = MDP.allocate_ğ©(Q)\n",
    "\n",
    "    Î”Q = -Inf\n",
    "    maxiter = 10000\n",
    "    iters_no = maxiter\n",
    "    for i = 1:maxiter\n",
    "        Î”Q = MDP.mk_update_Q!(Q, Î³, simulator, 1.0)\n",
    "        MDP.ğ©_from_Q!(ğ©, Q)\n",
    "        #! It does NOT WORK if we only check whether policy is changed or not!\n",
    "        if Î”Q < 1e-4\n",
    "            iters_no = i\n",
    "            break\n",
    "        end\n",
    "    end # for: iterations\n",
    "    return iters_no, Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_no, Q = optimize_policy_MK_incremental(Î³ = 0.95, Îµ = 0.05)\n",
    "\n",
    "V = MDP.allocate_V(fmdp)\n",
    "MDP.V_from_Q!(V, Q)\n",
    "Î”V = abs.(V - optimal_V)\n",
    "err = max(Î”V...)\n",
    "print(\"MK: policy optimization (incremental): max abs err = $err (final Î”Q = $Î”Q) after $iters_no iters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_to_matrix_form(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = MDP.allocate_ğ©(Q)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Î³_array = [0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "for Î³ in Î³_array\n",
    "    iters_no, Q = optimize_policy_MK_incremental(Î³ = Î³, Îµ = 0.05)\n",
    "    V = MDP.allocate_V(fmdp)\n",
    "    MDP.V_from_Q!(V, Q)\n",
    "    Î”V = abs.(V - optimal_V)\n",
    "    err = max(Î”V...)\n",
    "    println(\"MK: policy optimization (incremental Î³ = $Î³): max abs err = $err after $iters_no iters.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = MDP.allocate_ğ©(Q)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MDP.allocate_V(fmdp)\n",
    "Q = MDP.allocate_Q(fmdp)\n",
    "V[MDP.terminal_state(fmdp)] = 0\n",
    "\n",
    "compute_Î” = V_ -> max.((abs.(V_ - optimal_V))...)\n",
    "\n",
    "simulator = MDP.create_simulator_from_Q(fmdp, Q, 0.05, 1000)\n",
    "\n",
    "maxiter = 10000\n",
    "iters_no = maxiter\n",
    "\n",
    "Î´ = zeros(iters_no+1)\n",
    "Î” = zeros(iters_no+1)\n",
    "\n",
    "Î´[1] = NaN\n",
    "Î”[1] = compute_Î”(V)\n",
    "\n",
    "for i in 1:maxiter\n",
    "    Î´[i+1] = MDP.mk_update_Q!(Q, 0.95, simulator, 1.0)\n",
    "    MDP.V_from_Q!(V, Q)\n",
    "    Î”[i+1] = compute_Î”(V)\n",
    "end # for: iterations\n",
    "\n",
    "plot(Î”, label=\"error\")\n",
    "plot!(Î´, label=\"incremental diff.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ğ© = MDP.allocate_ğ©(Q)\n",
    "MDP.ğ©_from_Q!(ğ©, Q)\n",
    "ğ©_to_matrix_form(ğ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
